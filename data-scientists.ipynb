{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Science Notebook\n",
    "\n",
    "Build, train and serialize the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# load data\n",
    "from submodules.load_data import load_data\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# model\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# k-fold cross validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# saving models\n",
    "import joblib\n",
    "\n",
    "# performance\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data\n",
    "\n",
    "Load semi-colon separated data from disk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a Test Dataset\n",
    "> uses scikit-learn\n",
    "\n",
    "Performing this early minimizes generalization and bias you may inadvertently apply to your system.\n",
    "Simply put, a test set of data involves: picking ~20% of the instances randomly and setting them aside.\n",
    "\n",
    "Some considerations for sampling methods that generate the test set:\n",
    "1. you don't want your model to see the entire dataset\n",
    "1. you want to be able to fetch new data for training\n",
    "1. you want to maintain the same percentage of training data against the entire dataset\n",
    "1. you want a representative training dataset (~7% septic positive)\n",
    "\n",
    "https://realpython.com/train-test-split-python-data/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# sets 20% of the data aside for testing, sets the random number generate to it always generates the same shuffled indicies\n",
    "# x = 2 dimensional array with inputs\n",
    "# X_train is the training part of the first sequence (x)\n",
    "# X_test is the test part of the first sequence (x)\n",
    "# y = 1 dimensional array with outputs\n",
    "# y_train is the labeled training part of the second sequence\n",
    "# y_test is the labeled test part of the second sequence\n",
    "# axis Whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’)\n",
    "# test_size is the amount of the total dataset to set aside for testing = 10%\n",
    "# random state fixes the randomization so you get the same results each time\n",
    "# Shuffle before the data is split, it is shuffled\n",
    "# stratified splitting keeps the proportion of y values trhough the train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(data.drop(\"isSepsis\", axis=1),\n",
    "    data[\"isSepsis\"], test_size=0.1,\n",
    "    random_state=42, stratify=data[\"isSepsis\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data for Machine Learning\n",
    "Instead of preparing data manually, write functions to:\n",
    "1. reproduce transformations easily on any dataset (e.g., data refresh)\n",
    "1. builds a library of functions to reuse in future projects\n",
    "1. use functions in live stream to transform new data before inferencing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data cleaning\n",
    "1. split numerical attributes\n",
    "    1. transform current and future null values\n",
    "    1. drop under-represented attributes (<7k)\n",
    "    1. impute median for missing attributes (>7k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Copy Numeric Training Data, drop categorical and under-represented data\n",
    "1. under-rep. Unit1\n",
    "1. under-rep. Unit2\n",
    "1. under-rep. Gender\n",
    "1. drop Bilirubin_direct\n",
    "1. drop EtCO2\n",
    "1. drop Fibrinogen\n",
    "1. drop Troponin I\n",
    "1. drop Lactate\n",
    "1. drop SaO2\n",
    "1. drop FiO2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "          HR  O2Sat  Temp    SBP   MAP   DBP  Resp  BaseExcess  HCO3    pH  \\\n17166   90.0  100.0  37.3  108.5  79.5  67.5  10.0         0.0  23.0  7.33   \n8962    91.0    NaN   NaN  137.0  82.0   NaN  21.0         NaN  27.0   NaN   \n10453    NaN    NaN   NaN    NaN   NaN   NaN   NaN         NaN  25.0   NaN   \n19087   66.0  100.0  37.6  124.0  78.0  58.0  16.0         NaN   NaN   NaN   \n5328   103.0  100.0   NaN  108.0  82.0  68.0   NaN         NaN  25.0   NaN   \n\n       ...  Glucose  Magnesium  Phosphate  Potassium  Bilirubin_total   Hct  \\\n17166  ...     90.0        NaN        NaN        3.7              NaN  33.7   \n8962   ...    102.0        2.1        3.8        4.3              0.7  40.4   \n10453  ...     99.0        2.1        4.2        5.1              0.5  31.4   \n19087  ...    130.0        1.7        NaN        4.1              NaN  26.9   \n5328   ...     94.0        1.7        2.4        3.5              NaN  23.7   \n\n        Hgb   PTT   WBC  Platelets  \n17166  11.6  34.2  24.0      157.0  \n8962   13.9  38.6   8.3      209.0  \n10453   9.6   NaN   7.2      585.0  \n19087   9.0   NaN  14.2      179.0  \n5328    8.4  30.5  11.2      167.0  \n\n[5 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>HR</th>\n      <th>O2Sat</th>\n      <th>Temp</th>\n      <th>SBP</th>\n      <th>MAP</th>\n      <th>DBP</th>\n      <th>Resp</th>\n      <th>BaseExcess</th>\n      <th>HCO3</th>\n      <th>pH</th>\n      <th>...</th>\n      <th>Glucose</th>\n      <th>Magnesium</th>\n      <th>Phosphate</th>\n      <th>Potassium</th>\n      <th>Bilirubin_total</th>\n      <th>Hct</th>\n      <th>Hgb</th>\n      <th>PTT</th>\n      <th>WBC</th>\n      <th>Platelets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17166</th>\n      <td>90.0</td>\n      <td>100.0</td>\n      <td>37.3</td>\n      <td>108.5</td>\n      <td>79.5</td>\n      <td>67.5</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>23.0</td>\n      <td>7.33</td>\n      <td>...</td>\n      <td>90.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.7</td>\n      <td>NaN</td>\n      <td>33.7</td>\n      <td>11.6</td>\n      <td>34.2</td>\n      <td>24.0</td>\n      <td>157.0</td>\n    </tr>\n    <tr>\n      <th>8962</th>\n      <td>91.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>137.0</td>\n      <td>82.0</td>\n      <td>NaN</td>\n      <td>21.0</td>\n      <td>NaN</td>\n      <td>27.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>102.0</td>\n      <td>2.1</td>\n      <td>3.8</td>\n      <td>4.3</td>\n      <td>0.7</td>\n      <td>40.4</td>\n      <td>13.9</td>\n      <td>38.6</td>\n      <td>8.3</td>\n      <td>209.0</td>\n    </tr>\n    <tr>\n      <th>10453</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>25.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>99.0</td>\n      <td>2.1</td>\n      <td>4.2</td>\n      <td>5.1</td>\n      <td>0.5</td>\n      <td>31.4</td>\n      <td>9.6</td>\n      <td>NaN</td>\n      <td>7.2</td>\n      <td>585.0</td>\n    </tr>\n    <tr>\n      <th>19087</th>\n      <td>66.0</td>\n      <td>100.0</td>\n      <td>37.6</td>\n      <td>124.0</td>\n      <td>78.0</td>\n      <td>58.0</td>\n      <td>16.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>130.0</td>\n      <td>1.7</td>\n      <td>NaN</td>\n      <td>4.1</td>\n      <td>NaN</td>\n      <td>26.9</td>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>14.2</td>\n      <td>179.0</td>\n    </tr>\n    <tr>\n      <th>5328</th>\n      <td>103.0</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>108.0</td>\n      <td>82.0</td>\n      <td>68.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>25.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>94.0</td>\n      <td>1.7</td>\n      <td>2.4</td>\n      <td>3.5</td>\n      <td>NaN</td>\n      <td>23.7</td>\n      <td>8.4</td>\n      <td>30.5</td>\n      <td>11.2</td>\n      <td>167.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 27 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_num = X_train.copy()\n",
    "# .drop also creates a copy without the categorical attributes\n",
    "# drop non-biological indicators\n",
    "data_num = X_train.drop([\"SBP\",\n",
    "                         \"DBP\",\n",
    "                         \"EtCO2\",\n",
    "                         \"BaseExcess\",\n",
    "                         \"Magnesium\",\n",
    "                         \"Bilirubin_direct\",\n",
    "                         \"EtCO2\",\n",
    "                         \"Fibrinogen\",\n",
    "                         \"TroponinI\",\n",
    "                         \"Lactate\",\n",
    "                         \"SaO2\",\n",
    "                         \"FiO2\",\n",
    "                         \"Age\",\n",
    "                         \"Gender\",\n",
    "                         \"Unit1\",\n",
    "                         \"Unit2\",\n",
    "                         \"HospAdmTime\",\n",
    "                         \"ICULOS\"], axis=1)\n",
    "data_num.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transform missing values from numeric data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 82.  ,  98.  ,  36.8 , 120.  ,  80.  ,  62.  ,  18.  ,   0.  ,\n        24.  ,   7.39,  41.  ,  32.  ,  16.  ,  72.  ,   8.4 , 106.  ,\n         0.9 , 122.  ,   2.  ,   3.4 ,   4.  ,   0.8 ,  31.8 ,  10.7 ,\n        30.7 ,  10.1 , 193.  ,  63.25,  -6.08,  11.  ])"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create simpleimputer instance\n",
    "# replace attributes missing values with median of the attribute\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# fit applies the imputer to ALL numeric data in case new data includes null values\n",
    "# when system goes live\n",
    "# results are stored in a imputer.statistics_ value\n",
    "num_imputer.fit(data_num)\n",
    "num_imputer.statistics_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "          HR  O2Sat  Temp    SBP   MAP   DBP  Resp  BaseExcess  HCO3    pH  \\\n17166   90.0  100.0  37.3  108.5  79.5  67.5  10.0         0.0  23.0  7.33   \n8962    91.0   98.0  36.8  137.0  82.0  62.0  21.0         0.0  27.0  7.39   \n10453   82.0   98.0  36.8  120.0  80.0  62.0  18.0         0.0  25.0  7.39   \n19087   66.0  100.0  37.6  124.0  78.0  58.0  16.0         0.0  24.0  7.39   \n5328   103.0  100.0  36.8  108.0  82.0  68.0  18.0         0.0  25.0  7.39   \n\n       ...  Potassium  Bilirubin_total   Hct   Hgb   PTT   WBC  Platelets  \\\n17166  ...        3.7              0.8  33.7  11.6  34.2  24.0      157.0   \n8962   ...        4.3              0.7  40.4  13.9  38.6   8.3      209.0   \n10453  ...        5.1              0.5  31.4   9.6  30.7   7.2      585.0   \n19087  ...        4.1              0.8  26.9   9.0  30.7  14.2      179.0   \n5328   ...        3.5              0.8  23.7   8.4  30.5  11.2      167.0   \n\n         Age  HospAdmTime  ICULOS  \n17166  61.19        -3.62     4.0  \n8962   88.95       -62.83    16.0  \n10453  79.69        -0.02    14.0  \n19087  72.00       -31.88     9.0  \n5328   21.66        -0.03    21.0  \n\n[5 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>HR</th>\n      <th>O2Sat</th>\n      <th>Temp</th>\n      <th>SBP</th>\n      <th>MAP</th>\n      <th>DBP</th>\n      <th>Resp</th>\n      <th>BaseExcess</th>\n      <th>HCO3</th>\n      <th>pH</th>\n      <th>...</th>\n      <th>Potassium</th>\n      <th>Bilirubin_total</th>\n      <th>Hct</th>\n      <th>Hgb</th>\n      <th>PTT</th>\n      <th>WBC</th>\n      <th>Platelets</th>\n      <th>Age</th>\n      <th>HospAdmTime</th>\n      <th>ICULOS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17166</th>\n      <td>90.0</td>\n      <td>100.0</td>\n      <td>37.3</td>\n      <td>108.5</td>\n      <td>79.5</td>\n      <td>67.5</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>23.0</td>\n      <td>7.33</td>\n      <td>...</td>\n      <td>3.7</td>\n      <td>0.8</td>\n      <td>33.7</td>\n      <td>11.6</td>\n      <td>34.2</td>\n      <td>24.0</td>\n      <td>157.0</td>\n      <td>61.19</td>\n      <td>-3.62</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>8962</th>\n      <td>91.0</td>\n      <td>98.0</td>\n      <td>36.8</td>\n      <td>137.0</td>\n      <td>82.0</td>\n      <td>62.0</td>\n      <td>21.0</td>\n      <td>0.0</td>\n      <td>27.0</td>\n      <td>7.39</td>\n      <td>...</td>\n      <td>4.3</td>\n      <td>0.7</td>\n      <td>40.4</td>\n      <td>13.9</td>\n      <td>38.6</td>\n      <td>8.3</td>\n      <td>209.0</td>\n      <td>88.95</td>\n      <td>-62.83</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>10453</th>\n      <td>82.0</td>\n      <td>98.0</td>\n      <td>36.8</td>\n      <td>120.0</td>\n      <td>80.0</td>\n      <td>62.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>25.0</td>\n      <td>7.39</td>\n      <td>...</td>\n      <td>5.1</td>\n      <td>0.5</td>\n      <td>31.4</td>\n      <td>9.6</td>\n      <td>30.7</td>\n      <td>7.2</td>\n      <td>585.0</td>\n      <td>79.69</td>\n      <td>-0.02</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>19087</th>\n      <td>66.0</td>\n      <td>100.0</td>\n      <td>37.6</td>\n      <td>124.0</td>\n      <td>78.0</td>\n      <td>58.0</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>24.0</td>\n      <td>7.39</td>\n      <td>...</td>\n      <td>4.1</td>\n      <td>0.8</td>\n      <td>26.9</td>\n      <td>9.0</td>\n      <td>30.7</td>\n      <td>14.2</td>\n      <td>179.0</td>\n      <td>72.00</td>\n      <td>-31.88</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>5328</th>\n      <td>103.0</td>\n      <td>100.0</td>\n      <td>36.8</td>\n      <td>108.0</td>\n      <td>82.0</td>\n      <td>68.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>25.0</td>\n      <td>7.39</td>\n      <td>...</td>\n      <td>3.5</td>\n      <td>0.8</td>\n      <td>23.7</td>\n      <td>8.4</td>\n      <td>30.5</td>\n      <td>11.2</td>\n      <td>167.0</td>\n      <td>21.66</td>\n      <td>-0.03</td>\n      <td>21.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the trained imputer to transform the training set replacing the\n",
    "# missing values with learn medians\n",
    "N = num_imputer.transform(data_num)\n",
    "# result above is plain NumPy array with transformed features\n",
    "# put back to a pandas DataFrame\n",
    "num_tr = pd.DataFrame(N, columns=data_num.columns, index=data_num.index)\n",
    "num_tr.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Copy Categorical Training Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "       Gender\n17166       1\n8962        1\n10453       0\n19087       0\n5328        0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17166</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8962</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10453</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19087</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5328</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_cat = X_train.copy()\n",
    "# .drop also creates a copy without the categorical attributes\n",
    "data_cat = X_train[[\"Gender\"]]\n",
    "data_cat.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transform missing values from numeric data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create simpleimputer instance\n",
    "# replace missing using the most frequent value along each column\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# fit applies the imputer to ALL numeric data in case new data includes null values\n",
    "# when system goes live\n",
    "# results are stored in a imputer.statistics_ value\n",
    "cat_imputer.fit(data_cat)\n",
    "cat_imputer.statistics_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "       Gender\n17166       1\n8962        1\n10453       0\n19087       0\n5328        0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17166</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8962</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10453</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19087</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5328</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the trained imputer to transform the training set replacing the\n",
    "# missing values with learn medians\n",
    "C = cat_imputer.transform(data_cat)\n",
    "# result above is plain NumPy array with transformed features\n",
    "# put back to a pandas DataFrame\n",
    "cat_tr = pd.DataFrame(C, columns=data_cat.columns, index=data_cat.index)\n",
    "cat_tr.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Scaling\n",
    "1. ML algorithms don't work well when numeric attributes have very different scales\n",
    "    (e.g. HR max 184,  pH max 7.67)\n",
    "1. Scaling target values is not necessary\n",
    "1. Apply\n",
    "    1. normalization (MinMaxScaler) bounds the values to a specific range (e.g. 0-1)\n",
    "    1. standardization (StandardScaler) less affected by outliers does not bound to range\n",
    "\n",
    "### Transformation Pipeline\n",
    "\n",
    "1. Common to apply many transformation steps in a specific order"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "                        ('imputer', SimpleImputer(strategy='median')),\n",
    "                        ('std_scaler', StandardScaler()),\n",
    "                        ])\n",
    "\n",
    "num_prepared = num_pipeline.fit_transform(data_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OrdinalEncoder())\n",
    "])\n",
    "\n",
    "cat_prepared = cat_pipeline.fit_transform(data_cat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Data Pipeline\n",
    "\n",
    "Single transformer to handle numeric and categorical columns using ColumnTransformer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# get a list of numeric column names\n",
    "num_attribs = list(data_num)\n",
    "# get a list of categorical column names\n",
    "cat_attribs = [\"Gender\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# construct the transformer\n",
    "full_pipeline = ColumnTransformer([\n",
    "    # transform number columns with num_pipeline defined earlier\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    # transform categorical columns with cat_pipeline defined earler\n",
    "    (\"cat\", cat_pipeline, cat_attribs)\n",
    "])\n",
    "\n",
    "# only run the pipeline on the training as the test data will be applied during the evaluation stage with the final model\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Selection\n",
    "\n",
    "![image](images/scikitlearn-choose-right-estimator.png)\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "## Classifier Comparison\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html?highlight=svm%20svc\n",
    "\n",
    "1. [Linear Support Vector Machine \"SVM\" Support Vector Classifier \"SVC\"](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "1. [Naive Bayes]()\n",
    "1. [K-Neighbors Classifier](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification) implements learning based on the  nearest neighbors of each query point, where  is an integer value specified by the user.\n",
    "1. [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "1. [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) a linear model for classification\n",
    "1. [Stochastic Gradient Descent \"SGD\" Classifier](https://scikit-learn.org/stable/modules/sgd.html#classification)\n",
    "1. [Neural Network Multi-Layer Perceptron Classifier](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron) a supervised learning algorithm that learns a function  by training on a dataset\n",
    "1. [XGBoost Classifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=xgbclassifier#xgboost.XGBClassifier)\n",
    "\n",
    "## Scoring\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html#\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "Validation output readings\"\n",
    "    - fit_time - the time for fitting the estimator on the train set for each cv split\n",
    "    - score_time - the time for scoring the estimator on the test set for each cv split\n",
    "    - test_score - The score array for test scores on each cv split\n",
    "    - train_score - The score array for train scores on each cv split.\n",
    "\n",
    "Best practice to save every model you experiment with so you can come back easily to any model.\n",
    "Save both the hyperparameters and trained parameter, as well as the cross-validation scores and predictions.\n",
    "This will allow you to easily compare scores across model types. Use Pickle or joblib libraries."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear SVM SVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   24.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([18.17841792, 17.97551918, 18.55389619]),\n 'score_time': array([1.167032  , 1.17491698, 1.19085693]),\n 'test_score': array([0.3935743 , 0.40119166, 0.3891675 ]),\n 'train_score': array([0.39360639, 0.4       , 0.39640539])}"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the SVM\n",
    "lin_svm = svm.SVC(kernel='linear', # linear, poly, rbf, sigmoid, precomputed\n",
    "                  C=1)  # Regularization parameter\n",
    "# fit the model to the data\n",
    "lin_svm.fit(X_train_prepared, y_train)\n",
    "# configure the cross validation\n",
    "cv_lin_svm = cross_validate(lin_svm, # estimator to fit\n",
    "                            X_train_prepared, # data to fit\n",
    "                            y_train, # target variable isSepsis\n",
    "                            n_jobs=-1, # use all the processors in parallel\n",
    "                            verbose=1, # verbosity level\n",
    "                            cv=3, # splitting strategy to compute the score N consecutive times with different splits\n",
    "                            scoring=\"f1\", # for binary targets\n",
    "                            return_train_score=True)\n",
    "\n",
    "# display the scoring\n",
    "cv_lin_svm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/lin_svm.pkl']"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(lin_svm, \"models/lin_svm.pkl\")\n",
    "# reference to load the model\n",
    "#lin_svm_loaded = joblib.load(\"model/lin_svm.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.02321601, 0.02409697, 0.0232892 ]),\n 'score_time': array([0.00846076, 0.00828505, 0.00849104]),\n 'test_score': array([0.47810026, 0.44353183, 0.49229075]),\n 'train_score': array([0.47433535, 0.47161804, 0.46266234])}"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_prepared, y_train)\n",
    "cv_n_bayes = cross_validate(gnb, # estimator to fit\n",
    "                            X_train_prepared, # data to fit\n",
    "                            y_train, # target variable isSepsis\n",
    "                            n_jobs=-1, # use all the processors in parallel\n",
    "                            verbose=1, # verbosity level\n",
    "                            cv=3, # splitting strategy to compute the score N consecutive times with different splits\n",
    "                            scoring=\"f1\", # for binary targets\n",
    "                            return_train_score=True)\n",
    "\n",
    "# display the scoring\n",
    "cv_n_bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/gnb.pkl']"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(gnb, \"models/gnb.pkl\")\n",
    "# reference to load the model\n",
    "#gnb_loaded = joblib.load(\"model/gnb.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K Nearest Neighbor Classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   24.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.01427698, 0.01236105, 0.01475787]),\n 'score_time': array([9.0135572 , 9.59114289, 9.94566917]),\n 'test_score': array([0.50137237, 0.48843663, 0.50693802]),\n 'train_score': array([0.50978607, 0.5301314 , 0.49700874])}"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X_train_prepared, y_train)\n",
    "cv_knn = cross_validate(knn,\n",
    "                        X_train_prepared,\n",
    "                        y_train,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1,\n",
    "                        cv=3,\n",
    "                        scoring=\"f1\",\n",
    "                        return_train_score=True)\n",
    "cv_knn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/knn.pkl']"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(knn, \"models/knn.pkl\")\n",
    "# reference to load the model\n",
    "#knn_loaded = joblib.load(\"model/knn.pkl\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "{'fit_time': array([0.25918412, 0.23456287, 0.22348976]),\n 'score_time': array([0.01054025, 0.01056385, 0.0103364 ]),\n 'test_score': array([0.73068592, 0.74277457, 0.73342939]),\n 'train_score': array([0.96341858, 0.96046662, 0.96475913])}"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=5, # The number of trees in the forest\n",
    "                            verbose=1)\n",
    "cv_rf = cross_validate(rf,\n",
    "                       X_train_prepared,\n",
    "                       y_train,\n",
    "                       n_jobs=-1,\n",
    "                       cv=3,\n",
    "                       scoring=\"f1\",\n",
    "                       return_train_score=True)\n",
    "cv_rf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/rf.pkl']"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(rf, \"models/rf.pkl\")\n",
    "# reference to load the model\n",
    "#rf_loaded = joblib.load(\"model/rf.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "- [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.31530023, 0.31319308, 0.2987752 , 0.28334689, 0.16984296]),\n 'score_time': array([0.00441599, 0.00363684, 0.00378966, 0.003901  , 0.00327706]),\n 'test_score': array([0.49462366, 0.50228311, 0.49165402, 0.4893617 , 0.50306748]),\n 'train_score': array([0.50057056, 0.49060943, 0.5026616 , 0.50171821, 0.49406358])}"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a variable to Logistic regression with verbosity\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", # liblinear is ideal for small datasets\n",
    "                             C=1, # regularization; smaller values specify stronger regularization\n",
    "                             verbose=2)\n",
    "log_reg.fit(X_train_prepared, y_train)\n",
    "cv_log_reg = cross_validate(log_reg,\n",
    "                            X_train_prepared, # attributes\n",
    "                            y_train, # labels isSepsis\n",
    "                            n_jobs=-1, # use all the processors in parallel\n",
    "                            verbose=1, # verbosity level\n",
    "                            cv=5, # splitting strategy to compute the score N consecutive times with different splits\n",
    "                            scoring=\"f1\", # for binary targets\n",
    "                            return_train_score=True) # computationally expensive, whether to include training scores on parameters impact\n",
    "cv_log_reg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "joblib.dump(log_reg, \"models/log_reg.pkl\")\n",
    "# reference to load the model\n",
    "#log_reg_loaded = joblib.load(\"model/log_reg.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SGDClassifier\n",
    "\n",
    "advantages\n",
    "- efficient\n",
    "\n",
    "disadvantages\n",
    "- sensitive to feature scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.36535096, 0.33189511, 0.70298386, 0.41409278, 0.35349202]),\n 'score_time': array([0.00510001, 0.00801516, 0.00588202, 0.01588702, 0.00373793]),\n 'test_score': array([0.47457627, 0.5129771 , 0.42492013, 0.5       , 0.50366032]),\n 'train_score': array([0.47655356, 0.49246231, 0.4494829 , 0.51081594, 0.50764749])}"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(loss=\"log\", # logistic regression\n",
    "                        penalty=\"elasticnet\",\n",
    "                        alpha=0.0001,\n",
    "                        max_iter=1000,\n",
    "                        shuffle=True,\n",
    "                        learning_rate='optimal')\n",
    "sgd_clf.fit(X_train_prepared, y_train)\n",
    "cv_sgd_clf = cross_validate(sgd_clf,\n",
    "                            X_train_prepared, # attributes\n",
    "                            y_train, # labels isSepsis\n",
    "                            n_jobs=-1, # use all the processors in parallel\n",
    "                            verbose=1, # verbosity level\n",
    "                            cv=5, # splitting strategy to compute the score N consecutive times with different splits\n",
    "                            scoring=\"f1\", # for binary targets\n",
    "                            return_train_score=True) # computationally expensive, whether to include training scores on parameters impact\n",
    "\n",
    "cv_sgd_clf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/sgd_clf.pkl']"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# serialize the model\n",
    "joblib.dump(sgd_clf, \"models/sgd_clf.pkl\")\n",
    "# reference load the model\n",
    "#sgd_clf_loaded = joblib.load(\"model/sgd_clf.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  8.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([170.67743087, 175.3386929 , 186.25447607]),\n 'score_time': array([0.02585816, 0.02407122, 0.0230298 ]),\n 'test_score': array([0.58815959, 0.55924765, 0.55717762]),\n 'train_score': array([0.99335233, 0.99905452, 0.99810845])}"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = MLPClassifier(solver='sgd', # For small datasets, ‘lbfgs’ can converge faster and perform better.\n",
    "                   activation='relu',\n",
    "                   max_iter=5000, # The solver iterates until convergence\n",
    "                   hidden_layer_sizes=(50,50,50,50), # The ith element represents the number of neurons in the ith hidden layer\n",
    "                   verbose=0,\n",
    "                   learning_rate=\"adaptive\") #  keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing\n",
    "cv_nn = cross_validate(nn,\n",
    "                       X_train_prepared,\n",
    "                       y_train,\n",
    "                       cv=3,\n",
    "                       scoring=\"f1\",\n",
    "                       return_train_score=True,\n",
    "                       verbose=1)\n",
    "cv_nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/nn.pkl']"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(nn, \"models/nn.pkl\")\n",
    "# reference to load the model\n",
    "#nn_loaded = joblib.load(\"model/nn.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost Classifier\n",
    "\n",
    "Learning task parameter = https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "{'fit_time': array([2.07458496, 1.16849709, 1.20430899]),\n 'score_time': array([0.01188397, 0.00971794, 0.01256514]),\n 'test_score': array([0.76801153, 0.78481013, 0.78847505]),\n 'train_score': array([0.98352345, 0.98350254, 0.98224477])}"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost = XGBClassifier(use_label_encoder=False, # removes user warning error\n",
    "                        booster=\"gbtree\")\n",
    "cv_xgboost = cross_validate(xgboost,\n",
    "                            X_train_prepared,\n",
    "                            y_train,\n",
    "                            cv=3,\n",
    "                            scoring=\"f1\",\n",
    "                            return_train_score=True,\n",
    "                            verbose=0)\n",
    "cv_xgboost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/xgb.pkl']"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(xgboost, \"models/xgb.pkl\")\n",
    "# reference to load the model\n",
    "#xgboost_loaded = joblib.load(\"models/xgboost.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tune the model\n",
    "Instead of messing with hyperparameters manually, GridSearchCV can be instructed to search hyperparameters\n",
    "and uses cross validation to evaluate the possible combinations.\n",
    "\n",
    "During this sampling cycle, you may go back to your pipeline and:\n",
    "1. drop uninformative features\n",
    "1. add extra features\n",
    "1. clean up outliers\n",
    "\n",
    "### Grid Search on Random Forest Classifier\n",
    "\n",
    "[sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest%20classifier#sklearn.ensemble.RandomForestClassifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n             param_grid=[{'max_features': [10, 15, 20, 30],\n                          'n_estimators': [3, 10, 30, 100]},\n                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n                          'n_estimators': [3, 10]}],\n             return_train_score=True, scoring='f1')"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_param_grid = [\n",
    "    {'n_estimators': [3, 10, 30, 100], # The number of trees in the forest.\n",
    "     'max_features': [10, 15, 20, 30]}, # The number of features to consider when looking for the best split\n",
    "    {'bootstrap': [False], # If False, the whole dataset is used to build each tree\n",
    "     'n_estimators': [3,10],\n",
    "     'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# the grid search will explore 4 x 4 combinations of Random Forest Classifier combinations of\n",
    "# n_estimators and max_features hyperparameters values\n",
    "# Then will try 2 x 3 combinations of hyperparameter values in the second dictionary\n",
    "# with the bootstrap set to false\n",
    "# overall, gridsearch will explore 16 + 6 = 22 combinations of RFC hyperparameters and train each modele 3 times\n",
    "rfc_grid_search = GridSearchCV(rfc,\n",
    "                           rfc_param_grid,\n",
    "                           cv=3,\n",
    "                           scoring='f1',\n",
    "                           return_train_score=True)\n",
    "\n",
    "rfc_grid_search.fit(X_train_prepared, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7699968013598836"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best score\n",
    "rfc_grid_search.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "{'max_features': 10, 'n_estimators': 100}"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best combination of parameters\n",
    "rfc_grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(max_features=10)"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best estimator directly\n",
    "rfc_grid_search.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.37735773040100734, 'ICULOS'),\n (0.04730115690408714, 'WBC'),\n (0.04592894807237559, 'HospAdmTime'),\n (0.0430434397655355, 'BUN'),\n (0.04199322766534698, 'Creatinine'),\n (0.04199091114740825, 'Hct'),\n (0.036617303219033906, 'Temp'),\n (0.03510400759777423, 'Hgb'),\n (0.0348865190227094, 'Platelets'),\n (0.027757209337781624, 'HR'),\n (0.02109932050910739, 'Resp'),\n (0.02097147290351952, 'Age'),\n (0.01987236598516874, 'SBP'),\n (0.019530999972846838, 'MAP'),\n (0.019358680289891166, 'DBP'),\n (0.01827024793709712, 'Glucose'),\n (0.017085521159405256, 'Calcium'),\n (0.013463495831909837, 'Potassium'),\n (0.013366043383783787, 'O2Sat'),\n (0.011796822775000576, 'Magnesium'),\n (0.011171343157348778, 'HCO3'),\n (0.010928940185258838, 'Phosphate'),\n (0.01014210901381482, 'Chloride'),\n (0.009339537480688625, 'PTT'),\n (0.00916774467836452, 'Bilirubin_total'),\n (0.008296114192904555, 'pH'),\n (0.008258409034425615, 'Alkalinephos'),\n (0.008072103732817054, 'AST'),\n (0.007909745356941606, 'PaCO2'),\n (0.007092691966187609, 'BaseExcess'),\n (0.002825837320457614, 'Gender')]"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the importance scores next to the corresponding attributes\n",
    "# from this you can drop less useful features\n",
    "feature_importances = rfc_grid_search.best_estimator_.feature_importances_\n",
    "attributes = num_attribs + cat_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid Search on XGBoost\n",
    "[xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=xgbclassifier#xgboost.XGBClassifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=3,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, gamma=None,\n                                     gpu_id=None, importance_type='gain',\n                                     interaction_constraints=None,\n                                     learning_rate=None, max_delta_step=None,\n                                     max_depth=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     n_estimators=100, n_jobs...\n                                     num_parallel_tree=None, random_state=None,\n                                     reg_alpha=None, reg_lambda=None,\n                                     scale_pos_weight=None, subsample=None,\n                                     tree_method=None, use_label_encoder=False,\n                                     validate_parameters=None, verbosity=None),\n             param_grid={'alpha': [0, 0.1], 'max_delta_step': [0.1],\n                         'n_estimators': [150, 200], 'reg_lambda': [1, 1.1],\n                         'subsample': [None, 0.5, 1]},\n             return_train_score=True, scoring='f1')"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_param_grid = {\"n_estimators\": [150, 200],\n",
    "                  \"max_delta_step\": [0.1],\n",
    "                  \"subsample\": [None, 0.5, 1],\n",
    "                  \"reg_lambda\": [1, 1.1],\n",
    "                  \"alpha\": [0, 0.1]}\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "xgb_grid_search = GridSearchCV(xgb,\n",
    "                           xgb_param_grid,\n",
    "                           cv=3,\n",
    "                           scoring='f1',\n",
    "                           return_train_score=True)\n",
    "\n",
    "xgb_grid_search.fit(X_train_prepared, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7582144265457043"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best score\n",
    "xgb_grid_search.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "{'alpha': 0,\n 'max_delta_step': 0.1,\n 'n_estimators': 200,\n 'reg_lambda': 1.1,\n 'subsample': None}"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best combination of parameters\n",
    "xgb_grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "XGBClassifier(alpha=0, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0.1, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=200, n_jobs=12, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1.1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', use_label_encoder=False,\n              validate_parameters=1, verbosity=None)"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best estimator directly\n",
    "xgb_grid_search.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.17644046, 'ICULOS'),\n (0.15313622, 'BUN'),\n (0.12117299, 'WBC'),\n (0.06759909, 'Hgb'),\n (0.04747762, 'Creatinine'),\n (0.046846576, 'HospAdmTime'),\n (0.041359287, 'Platelets'),\n (0.026133139, 'Temp'),\n (0.023732722, 'HR'),\n (0.022136042, 'HCO3'),\n (0.017808491, 'Calcium'),\n (0.017374972, 'Hct'),\n (0.017354779, 'DBP'),\n (0.017209586, 'Chloride'),\n (0.016534038, 'PTT'),\n (0.015087422, 'Magnesium'),\n (0.015005713, 'Resp'),\n (0.014618657, 'pH'),\n (0.0141673405, 'O2Sat'),\n (0.013776249, 'Phosphate'),\n (0.0133996075, 'Potassium'),\n (0.013325603, 'SBP'),\n (0.013107742, 'Glucose'),\n (0.01257483, 'BaseExcess'),\n (0.011553415, 'MAP'),\n (0.01104402, 'Bilirubin_total'),\n (0.010610401, 'Alkalinephos'),\n (0.010555101, 'Age'),\n (0.009997199, 'PaCO2'),\n (0.0074839913, 'AST'),\n (0.0013767334, 'Gender')]"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = xgb_grid_search.best_estimator_.feature_importances_\n",
    "attributes = num_attribs + cat_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate RFC model on the Test set\n",
    "\n",
    "After tweaking the model, you can evaluate the final model on the test set.\n",
    "1. get the predictors and labels from your test set\n",
    "1. run the transformation pipeline with transform() < don't fit the test set\n",
    "1. run the prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform, DON'T fit the final data\n",
    "X_test_ready = full_pipeline.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "final_model = rfc_grid_search.best_estimator_\n",
    "\n",
    "final_predictions = final_model.predict(X_test_ready)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "0.952905535665106"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the percent of the predictions that were correct\n",
    "accuracy_score(y_test, final_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate XGB model on the Test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'XGBClassifier' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-171-2ae5cf6e760a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mfinal_model_x\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mxgb_grid_search\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mx_final_predictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfinal_model_x\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test_ready\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'XGBClassifier' object is not callable"
     ]
    }
   ],
   "source": [
    "final_model_x = xgb_grid_search.best_estimator_\n",
    "\n",
    "x_final_predictions = final_model_x(X_test_ready)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# view the accuracy\n",
    "accuracy_score(y_test, x_final_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}