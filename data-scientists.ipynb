{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Science Notebook\n",
    "\n",
    "Build, train and serialize the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# load data\n",
    "from submodules.load_data import load_data\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from submodules.plots import plotGender\n",
    "from submodules.plots import plotUnit\n",
    "\n",
    "\n",
    "# data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# data preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# model\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# performance\n",
    "from sklearn.metrics import f1_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data\n",
    "\n",
    "Load semi-colon seperated data from disk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a Test Dataset\n",
    "> uses scikit-learn\n",
    "\n",
    "Performing this early minimizes generalization and bias you may inadvertently apply to your system.\n",
    "Simply put, a test set of data involves: picking ~20% of the instances randomly and setting them aside.\n",
    "\n",
    "Some considerations for sampling methods that generate the test set:\n",
    "1. you don't want your model to see the entire dataset\n",
    "1. you want to be able to fetch new data for training\n",
    "1. you want to maintain the same percentage of training data against the entire dataset\n",
    "1. you want a representative training dataset (~7% septic positive)\n",
    "\n",
    "https://realpython.com/train-test-split-python-data/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# sets 20% of the data aside for testing, sets the random number generate to it always generates the same shuffled indicies\n",
    "# x = 2 dimensional array with inputs\n",
    "# X_train is the training part of the first sequence (x)\n",
    "# X_test is the test part of the first sequence (x)\n",
    "# y = 1 dimensional array with outputs\n",
    "# y_train is the labeled training part of the second sequence\n",
    "# y_test is the labeled test part of the second sequence\n",
    "# test_size is the amount of the total dataset to set aside for testing\n",
    "# random state fixes the randomization so you get the same results each time\n",
    "# Shuffle before the data is split, it is shuffled\n",
    "# stratified splitting keeps the proportion of y values trhough the train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(data.drop(\"isSepsis\", axis=1),\n",
    "    data[\"isSepsis\"], test_size=0.2,\n",
    "    random_state=42, stratify=data[\"isSepsis\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Age\n",
    "- divide into age groups for both training and test datasets\n",
    "- under 19 years old\n",
    "- 19 to 65 years old\n",
    "- 65 years and older"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# X_train is all the instance with attributes\n",
    "X_train = X_train.loc[X_train[\"Age\"] <= 100]\n",
    "# y_train is the label of each instance (isSepsis = 1 or 0)\n",
    "y_train = y_train.loc[X_train[\"Age\"] <= 100]\n",
    "\n",
    "# X_test is all the instance with attributes\n",
    "X_test = X_test.loc[X_test[\"Age\"] <= 100]\n",
    "# y_test is the label of each instance (isSepsis = 1 or 0)\n",
    "y_test = y_test.loc[X_test[\"Age\"] <= 100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning & Transformation Pipeline\n",
    "\n",
    "## Age\n",
    "- divide age into categories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Age data before transforming:  (29041,)\n"
     ]
    }
   ],
   "source": [
    "# see the shape of the data before transformation\n",
    "print(\"Shape of Age data before transforming: \", X_train[\"Age\"].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(29041, 1)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# discretization is the process of transferring continuous\n",
    "# functions, models, variables, and equations into discrete\n",
    "# counterparts. This process is usually carried out as a first\n",
    "# step toward making them suitable for numerical evaluation\n",
    "# and implementation on digital computers.\n",
    "# this splits age into 4 categories\n",
    "def discretizateAge(data):\n",
    "    # teen, youth, adult, senior\n",
    "    bins = [13, 18, 30, 60, np.inf]\n",
    "    data = np.digitize(data, bins=bins)\n",
    "    data = data.reshape(len(data), 1)\n",
    "    return data\n",
    "\n",
    "# from scikit-learn\n",
    "# transform() method transforms the entire dataset\n",
    "# which is passes as a parameter to the function\n",
    "DiscretizateAge = FunctionTransformer(discretizateAge)\n",
    "# fits the transformation back to the dataset\n",
    "DiscretizateAge.fit_transform(X_train[\"Age\"]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(29041, 1)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn provides a pipeline class\n",
    "# create a class to replace missing values with the median of the column data\n",
    "# fit the imputer instance to the  data using fit() method\n",
    "age_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"discretizator\", DiscretizateAge)\n",
    "])\n",
    "\n",
    "# see the shape of the data\n",
    "# use the \"trained\" imputer to transform the training set by replacing missing values with the learned medians\n",
    "age_pipeline.fit_transform(X_train[[\"Age\"]]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Intensive Care Units\n",
    "\n",
    "Unit 1 and Unit 2\n",
    "Unit1 - Administrative identifier for ICU unit (MICU)\n",
    "Unit2 - Administrative identifier for ICU unit (SICU)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def CombineUnits(units_cols):\n",
    "    data = units_cols.copy()\n",
    "    data[\"Unit\"] = pd.Series(np.zeros((len(data))))\n",
    "    data.loc[data[\"Unit1\"] == 1, \"Unit\"] = \"MICU\"\n",
    "    data.loc[data[\"Unit2\"] == 1, \"Unit\"] = \"SICU\"\n",
    "    data.loc[(data[\"Unit1\"].isna()) & (data[\"Unit2\"].isna()), \"Unit\"] = \"Other ICU\"\n",
    "    return data[[\"Unit\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(29041, 3)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combineUnit1and2 = FunctionTransformer(CombineUnits)\n",
    "\n",
    "units = [\"Unit1\", \"Unit2\"]\n",
    "\n",
    "unit_pipeline = Pipeline([\n",
    "    (\"combine\", combineUnit1and2),\n",
    "    (\"encoder\", OneHotEncoder(sparse=False))\n",
    "])\n",
    "\n",
    "unit_pipeline.fit_transform(X_train[units]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acidbase_features = [\"BaseExcess\", \"PaCO2\"]\n",
    "\n",
    "def isAcidBaseDisturb(cols):\n",
    "    cols = np.c_[cols, np.zeros(len(cols))]\n",
    "    cols[:,2][(cols[:,0] < -2) & (cols[:,1] < 40)] = 1\n",
    "    col = cols[:,2].reshape(len(cols), 1)\n",
    "    return col\n",
    "\n",
    "FindAcidosis = FunctionTransformer(isAcidBaseDisturb)\n",
    "FindAcidosis.fit_transform(X_train[acidbase_features])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(29041, 1)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acidbase_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"acidosis\", FindAcidosis)\n",
    "])\n",
    "\n",
    "acidbase_pipeline.fit_transform(X_train[acidbase_features]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(29041, 18)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = [\"HR\",\n",
    "                \"O2Sat\",\n",
    "                \"Temp\",\n",
    "                \"MAP\",\n",
    "                \"Resp\",\n",
    "                \"AST\",\n",
    "                \"BUN\",\n",
    "                \"Alkalinephos\",\n",
    "                \"Calcium\",\n",
    "                \"Creatinine\",\n",
    "                \"Glucose\",\n",
    "                \"Bilirubin_total\",\n",
    "                \"Hgb\",\n",
    "                \"PTT\",\n",
    "                \"WBC\",\n",
    "                \"Fibrinogen\",\n",
    "                \"Platelets\",\n",
    "                \"ICULOS\"\n",
    "                ]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "num_pipeline.fit_transform(X_train[num_features]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.],\n       [0.],\n       [0.],\n       ...,\n       [1.],\n       [1.],\n       [0.]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OrdinalEncoder())\n",
    "])\n",
    "\n",
    "gender_pipeline.fit_transform(X_train[[\"Gender\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer] ....... (1 of 5) Processing numbers, total=   0.1s\n",
      "[ColumnTransformer] ...... (2 of 5) Processing acidbase, total=   0.0s\n",
      "[ColumnTransformer] ........... (3 of 5) Processing age, total=   0.0s\n",
      "[ColumnTransformer] ......... (4 of 5) Processing units, total=   0.0s\n",
      "[ColumnTransformer] ........ (5 of 5) Processing gender, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": "(29041, 24)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = ColumnTransformer([\n",
    "    (\"numbers\", num_pipeline, num_features),\n",
    "    (\"acidbase\", acidbase_pipeline, acidbase_features),\n",
    "    (\"age\", age_pipeline, [\"Age\"]),\n",
    "    (\"units\", unit_pipeline, units),\n",
    "    (\"gender\", gender_pipeline, [\"Gender\"])\n",
    "], verbose=True)\n",
    "\n",
    "preprocessing_pipeline.fit_transform(X_train).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Pipeline against Train and Test Datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer] ....... (1 of 5) Processing numbers, total=   0.1s\n",
      "[ColumnTransformer] ...... (2 of 5) Processing acidbase, total=   0.0s\n",
      "[ColumnTransformer] ........... (3 of 5) Processing age, total=   0.0s\n",
      "[ColumnTransformer] ......... (4 of 5) Processing units, total=   0.0s\n",
      "[ColumnTransformer] ........ (5 of 5) Processing gender, total=   0.0s\n",
      "[ColumnTransformer] ....... (1 of 5) Processing numbers, total=   0.0s\n",
      "[ColumnTransformer] ...... (2 of 5) Processing acidbase, total=   0.0s\n",
      "[ColumnTransformer] ........... (3 of 5) Processing age, total=   0.0s\n",
      "[ColumnTransformer] ......... (4 of 5) Processing units, total=   0.0s\n",
      "[ColumnTransformer] ........ (5 of 5) Processing gender, total=   0.0s\n"
     ]
    }
   ],
   "source": [
    "X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test = preprocessing_pipeline.fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Selection\n",
    "\n",
    "![image](images/scikirlearn-choose-right-estimator.png)\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "1. [Linear Support Vector Machine](https://scikit-learn.org/stable/modules/svm.html#classification)\n",
    "1. [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) a linear model for classification\n",
    "1. [K-Neighbors Classifier](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification) implements learning based on the  nearest neighbors of each query point, where  is an integer value specified by the user.\n",
    "1. [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "1. XG Boost Classifier\n",
    "1. [Neural Network Multi-Layer Perceptron Classifier](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron) a supervised learning algorithm that learns a function  by training on a dataset\n",
    "\n",
    "## Scoring\n",
    "Validation output readings\"\n",
    "    - fit_time - the time for fitting the estimator on the train set for each cv split\n",
    "    - score_time - the time for scoring the estimator on the test set for each cv split\n",
    "    - test_score - The score array for test scores on each cv split\n",
    "    - train_score - The score array for train scores on each cv split."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear SVM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "{'fit_time': array([4.07299304, 3.80910397, 3.8720181 ]),\n 'score_time': array([2.82740402, 2.72457409, 4.14820504]),\n 'test_score': array([0.48504274, 0.52111226, 0.45356371]),\n 'train_score': array([0.51609553, 0.50446663, 0.52691511])}"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the SVM\n",
    "lin_svm = svm.SVC()\n",
    "# fit the model to the data\n",
    "lin_svm.fit(X_train, y_train)\n",
    "# configure the cross valudation\n",
    "cv_lin_svm = cross_validate(lin_svm, # estimator to fit\n",
    "                            X_train, # data to fit\n",
    "                            y_train, # target variable isSepsis\n",
    "                            n_jobs=-1, # use all the processors in parallel\n",
    "                            verbose=1, # verbosity level\n",
    "                            cv=3, # splitting strategy to compute the score N consecutive times with different splits\n",
    "                            scoring=\"f1\", # for binary targets\n",
    "                            return_train_score=True)\n",
    "# display the scoring\n",
    "cv_lin_svm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K Nearest Neighbor Classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   25.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.01050782, 0.01596522, 0.0180161 ]),\n 'score_time': array([12.07834792, 11.73511076, 11.82910824]),\n 'test_score': array([0.4732334, 0.5046729, 0.430131 ]),\n 'train_score': array([0.48762507, 0.45665051, 0.48996832])}"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(X_train, y_train)\n",
    "cv_knn = cross_validate(knn,\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1,\n",
    "                        cv=3,\n",
    "                        scoring=\"f1\",\n",
    "                        return_train_score=True)\n",
    "cv_knn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Radius Neighbor Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/model_selection/_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/neighbors/_classification.py\", line 504, in predict\n",
      "    probs = self.predict_proba(X)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/neighbors/_classification.py\", line 564, in predict_proba\n",
      "    raise ValueError('No neighbors found for test samples %r, '\n",
      "ValueError: No neighbors found for test samples array([   0,    1,    2, ..., 9677, 9679, 9680]), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/model_selection/_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/neighbors/_classification.py\", line 504, in predict\n",
      "    probs = self.predict_proba(X)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/neighbors/_classification.py\", line 564, in predict_proba\n",
      "    raise ValueError('No neighbors found for test samples %r, '\n",
      "ValueError: No neighbors found for test samples array([   0,    1,    2, ..., 9677, 9678, 9679]), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/model_selection/_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/neighbors/_classification.py\", line 504, in predict\n",
      "    probs = self.predict_proba(X)\n",
      "  File \"/Users/davidmarcus/.virtualenvs/sepsisDetection /lib/python3.9/site-packages/sklearn/neighbors/_classification.py\", line 564, in predict_proba\n",
      "    raise ValueError('No neighbors found for test samples %r, '\n",
      "ValueError: No neighbors found for test samples array([   0,    1,    3, ..., 9677, 9678, 9679]), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   13.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.00887012, 0.00262618, 0.00242805]),\n 'score_time': array([1.95225811, 1.41093588, 1.39185286]),\n 'test_score': array([nan, nan, nan]),\n 'train_score': array([0.97461929, 0.97649186, 0.97167756])}"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RadiusNeighborsClassifier(radius=1.0)\n",
    "rnn.fit(X_train, y_train)\n",
    "cv_rnn = cross_validate(rnn,\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1,\n",
    "                        cv=3,\n",
    "                        scoring=\"f1\",\n",
    "                        return_train_score=True)\n",
    "cv_rnn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.98283792, 0.91681886, 0.94060397]),\n 'score_time': array([0.00798917, 0.00885034, 0.00783825]),\n 'test_score': array([0.63128877, 0.64996521, 0.62068966]),\n 'train_score': array([1., 1., 1.])}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "dct.fit(X_train, y_train)\n",
    "cv_dct = cross_validate(dct,\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1,\n",
    "                        cv=3,\n",
    "                        scoring=\"f1\",\n",
    "                        return_train_score=True)\n",
    "cv_dct"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "{'fit_time': array([0.25298214, 0.24710989, 0.24381614]),\n 'score_time': array([0.01477695, 0.01457   , 0.01472092]),\n 'test_score': array([0.73960984, 0.74333333, 0.72379368]),\n 'train_score': array([0.96296296, 0.96938776, 0.96526508])}"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, verbose=1)\n",
    "cv_rf = cross_validate(rf,\n",
    "                       X_train,\n",
    "                       y_train,\n",
    "                       n_jobs=-1,\n",
    "                       cv=3,\n",
    "                       scoring=\"f1\",\n",
    "                       return_train_score=True)\n",
    "cv_rf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extremely Randomized Trees"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "{'fit_time': array([0.176332  , 0.18742609, 0.18132997]),\n 'score_time': array([0.02071595, 0.024019  , 0.02474308]),\n 'test_score': array([0.64292409, 0.63450835, 0.60478469]),\n 'train_score': array([1., 1., 1.])}"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erf = ExtraTreesClassifier(n_estimators=10, verbose=1)\n",
    "cv_erf = cross_validate(erf,\n",
    "                       X_train,\n",
    "                       y_train,\n",
    "                       n_jobs=-1,\n",
    "                       cv=3,\n",
    "                       scoring=\"f1\",\n",
    "                       return_train_score=True)\n",
    "cv_erf\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "- [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression)\n",
    "- [Cross Validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html?highlight=cross_validate#sklearn-model-selection-cross-validate) to split the training set into k smaller sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.4997108 , 0.51584601, 0.5105648 ]),\n 'score_time': array([0.00688839, 0.00648093, 0.01709318]),\n 'test_score': array([0.50102669, 0.53532338, 0.46349206]),\n 'train_score': array([0.49354672, 0.49588477, 0.50740174])}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a variable to Logistic regression with verbosity\n",
    "log_reg = LogisticRegression(verbose=2)\n",
    "log_reg.fit(X_train, y_train)\n",
    "cv_log_reg = cross_validate(log_reg,\n",
    "                            X_train, # attributes\n",
    "                            y_train, # labels isSepsis\n",
    "                            n_jobs=-1, # use all the processors in parallel\n",
    "                            verbose=1, # verbosity level\n",
    "                            cv=3, # splitting strategy to compute the score N consecutive times with different splits\n",
    "                            scoring=\"f1\", # for binary targets\n",
    "                            return_train_score=True) # computationally expensive, whether to include training scores on parameters impact\n",
    "cv_log_reg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([2.38287592, 1.11645913, 1.46486402]),\n 'score_time': array([0.01061916, 0.01619792, 0.01234484]),\n 'test_score': array([0.75722543, 0.76086957, 0.74691806]),\n 'train_score': array([0.96514012, 0.96811793, 0.96978022])}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost = XGBClassifier(n_estimators=150, use_label_encoder=False, scale_pos_weight=12, eval_metric=\"aucpr\", verbosity=1, disable_default_eval_metric=1)\n",
    "cv_xgboost = cross_validate(xgboost,\n",
    "                            X_train,\n",
    "                            y_train,\n",
    "                            cv=3,\n",
    "                            scoring=\"f1\",\n",
    "                            return_train_score=True,\n",
    "                            verbose=1)\n",
    "cv_xgboost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([19.7811861 , 27.49344206, 32.61228228]),\n 'score_time': array([0.02718282, 0.02237582, 0.02795482]),\n 'test_score': array([0.62164361, 0.6309434 , 0.61001517]),\n 'train_score': array([0.94042713, 0.98717035, 0.99681416])}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = MLPClassifier(max_iter=5000, hidden_layer_sizes=(50,50,50,50), verbose=0, learning_rate=\"adaptive\")\n",
    "cv_nn = cross_validate(nn,\n",
    "                       X_train,\n",
    "                       y_train,\n",
    "                       cv=3,\n",
    "                       scoring=\"f1\",\n",
    "                       return_train_score=True,\n",
    "                       verbose=1)\n",
    "cv_nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}