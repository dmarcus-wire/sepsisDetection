{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Science Notebook\n",
    "\n",
    "Build, train and serialize the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# load data\n",
    "from submodules.load_data import load_data\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# model\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# k-fold cross validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# saving models\n",
    "import joblib\n",
    "\n",
    "# performance\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data\n",
    "\n",
    "Load semi-colon separated data from disk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a Test Dataset\n",
    "> uses scikit-learn\n",
    "\n",
    "Performing this early minimizes generalization and bias you may inadvertently apply to your system.\n",
    "Simply put, a test set of data involves: picking ~20% of the instances randomly and setting them aside.\n",
    "\n",
    "Some considerations for sampling methods that generate the test set:\n",
    "1. you don't want your model to see the entire dataset\n",
    "1. you want to be able to fetch new data for training\n",
    "1. you want to maintain the same percentage of training data against the entire dataset\n",
    "1. you want a representative training dataset (~7% septic positive)\n",
    "\n",
    "https://realpython.com/train-test-split-python-data/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# sets 15% of the data aside for testing, sets the random number generate to it always generates the same shuffled indicies\n",
    "# x = 2 dimensional array with inputs\n",
    "# X_train is the training part of the first sequence (x)\n",
    "# X_test is the test part of the first sequence (x)\n",
    "# y = 1 dimensional array with outputs\n",
    "# y_train is the labeled training part of the second sequence\n",
    "# y_test is the labeled test part of the second sequence\n",
    "# axis Whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’)\n",
    "# test_size is the amount of the total dataset to set aside for testing = 10%\n",
    "# random state fixes the randomization so you get the same results each time\n",
    "# Shuffle before the data is split, it is shuffled\n",
    "# stratified splitting keeps the proportion of y values trhough the train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(data.drop(\"isSepsis\", axis=1),\n",
    "    data[\"isSepsis\"], test_size=0.15,\n",
    "    random_state=42, stratify=data[\"isSepsis\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data for Machine Learning\n",
    "Instead of preparing data manually, write functions to:\n",
    "1. reproduce transformations easily on any dataset (e.g., data refresh)\n",
    "1. builds a library of functions to reuse in future projects\n",
    "1. use functions in live stream to transform new data before inferencing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data cleaning\n",
    "1. split numerical attributes\n",
    "    1. transform current and future null values\n",
    "    1. drop under-represented attributes (<7k)\n",
    "    1. impute median for missing attributes (>7k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Copy Numeric Training Data, drop demographics data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "          HR  O2Sat  Temp    SBP   MAP   DBP  Resp  EtCO2  BaseExcess  HCO3  \\\n17166   90.0  100.0  37.3  108.5  79.5  67.5  10.0    NaN         0.0  23.0   \n8962    91.0    NaN   NaN  137.0  82.0   NaN  21.0    NaN         NaN  27.0   \n10453    NaN    NaN   NaN    NaN   NaN   NaN   NaN    NaN         NaN  25.0   \n19087   66.0  100.0  37.6  124.0  78.0  58.0  16.0   31.0         NaN   NaN   \n5328   103.0  100.0   NaN  108.0  82.0  68.0   NaN    NaN         NaN  25.0   \n\n       ...  Phosphate  Potassium  Bilirubin_total  TroponinI   Hct   Hgb  \\\n17166  ...        NaN        3.7              NaN        NaN  33.7  11.6   \n8962   ...        3.8        4.3              0.7        NaN  40.4  13.9   \n10453  ...        4.2        5.1              0.5        NaN  31.4   9.6   \n19087  ...        NaN        4.1              NaN        NaN  26.9   9.0   \n5328   ...        2.4        3.5              NaN        NaN  23.7   8.4   \n\n        PTT   WBC  Fibrinogen  Platelets  \n17166  34.2  24.0         NaN      157.0  \n8962   38.6   8.3         NaN      209.0  \n10453   NaN   7.2         NaN      585.0  \n19087   NaN  14.2         NaN      179.0  \n5328   30.5  11.2         NaN      167.0  \n\n[5 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>HR</th>\n      <th>O2Sat</th>\n      <th>Temp</th>\n      <th>SBP</th>\n      <th>MAP</th>\n      <th>DBP</th>\n      <th>Resp</th>\n      <th>EtCO2</th>\n      <th>BaseExcess</th>\n      <th>HCO3</th>\n      <th>...</th>\n      <th>Phosphate</th>\n      <th>Potassium</th>\n      <th>Bilirubin_total</th>\n      <th>TroponinI</th>\n      <th>Hct</th>\n      <th>Hgb</th>\n      <th>PTT</th>\n      <th>WBC</th>\n      <th>Fibrinogen</th>\n      <th>Platelets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17166</th>\n      <td>90.0</td>\n      <td>100.0</td>\n      <td>37.3</td>\n      <td>108.5</td>\n      <td>79.5</td>\n      <td>67.5</td>\n      <td>10.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>3.7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>33.7</td>\n      <td>11.6</td>\n      <td>34.2</td>\n      <td>24.0</td>\n      <td>NaN</td>\n      <td>157.0</td>\n    </tr>\n    <tr>\n      <th>8962</th>\n      <td>91.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>137.0</td>\n      <td>82.0</td>\n      <td>NaN</td>\n      <td>21.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>27.0</td>\n      <td>...</td>\n      <td>3.8</td>\n      <td>4.3</td>\n      <td>0.7</td>\n      <td>NaN</td>\n      <td>40.4</td>\n      <td>13.9</td>\n      <td>38.6</td>\n      <td>8.3</td>\n      <td>NaN</td>\n      <td>209.0</td>\n    </tr>\n    <tr>\n      <th>10453</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>25.0</td>\n      <td>...</td>\n      <td>4.2</td>\n      <td>5.1</td>\n      <td>0.5</td>\n      <td>NaN</td>\n      <td>31.4</td>\n      <td>9.6</td>\n      <td>NaN</td>\n      <td>7.2</td>\n      <td>NaN</td>\n      <td>585.0</td>\n    </tr>\n    <tr>\n      <th>19087</th>\n      <td>66.0</td>\n      <td>100.0</td>\n      <td>37.6</td>\n      <td>124.0</td>\n      <td>78.0</td>\n      <td>58.0</td>\n      <td>16.0</td>\n      <td>31.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>26.9</td>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>14.2</td>\n      <td>NaN</td>\n      <td>179.0</td>\n    </tr>\n    <tr>\n      <th>5328</th>\n      <td>103.0</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>108.0</td>\n      <td>82.0</td>\n      <td>68.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>25.0</td>\n      <td>...</td>\n      <td>2.4</td>\n      <td>3.5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>23.7</td>\n      <td>8.4</td>\n      <td>30.5</td>\n      <td>11.2</td>\n      <td>NaN</td>\n      <td>167.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_num = X_train.copy()\n",
    "# .drop also creates a copy without the categorical attributes\n",
    "# drop non-biological indicators\n",
    "data_num = X_train.drop([\"Age\",\n",
    "                         \"Unit1\",\n",
    "                         \"Unit2\",\n",
    "                         \"HospAdmTime\",\n",
    "                         \"ICULOS\",\n",
    "                         \"Gender\"\n",
    "                         ], axis=1)\n",
    "data_num.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transform missing values from numeric data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([8.20e+01, 9.80e+01, 3.68e+01, 1.20e+02, 8.00e+01, 6.20e+01,\n       1.80e+01, 3.30e+01, 0.00e+00, 2.40e+01, 5.00e-01, 7.39e+00,\n       4.10e+01, 9.70e+01, 3.20e+01, 1.60e+01, 7.20e+01, 8.40e+00,\n       1.06e+02, 9.00e-01, 2.80e-01, 1.22e+02, 1.80e+00, 2.00e+00,\n       3.40e+00, 4.00e+00, 8.00e-01, 1.40e-01, 3.18e+01, 1.07e+01,\n       3.07e+01, 1.01e+01, 2.51e+02, 1.93e+02])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create simpleimputer instance\n",
    "# replace attributes missing values with median of the attribute\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# fit applies the imputer to ALL numeric data in case new data includes null values\n",
    "# when system goes live\n",
    "# results are stored in a imputer.statistics_ value\n",
    "num_imputer.fit(data_num)\n",
    "num_imputer.statistics_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "          HR  O2Sat  Temp    SBP   MAP   DBP  Resp  EtCO2  BaseExcess  HCO3  \\\n17166   90.0  100.0  37.3  108.5  79.5  67.5  10.0   33.0         0.0  23.0   \n8962    91.0   98.0  36.8  137.0  82.0  62.0  21.0   33.0         0.0  27.0   \n10453   82.0   98.0  36.8  120.0  80.0  62.0  18.0   33.0         0.0  25.0   \n19087   66.0  100.0  37.6  124.0  78.0  58.0  16.0   31.0         0.0  24.0   \n5328   103.0  100.0  36.8  108.0  82.0  68.0  18.0   33.0         0.0  25.0   \n\n       ...  Phosphate  Potassium  Bilirubin_total  TroponinI   Hct   Hgb  \\\n17166  ...        3.4        3.7              0.8       0.14  33.7  11.6   \n8962   ...        3.8        4.3              0.7       0.14  40.4  13.9   \n10453  ...        4.2        5.1              0.5       0.14  31.4   9.6   \n19087  ...        3.4        4.1              0.8       0.14  26.9   9.0   \n5328   ...        2.4        3.5              0.8       0.14  23.7   8.4   \n\n        PTT   WBC  Fibrinogen  Platelets  \n17166  34.2  24.0       251.0      157.0  \n8962   38.6   8.3       251.0      209.0  \n10453  30.7   7.2       251.0      585.0  \n19087  30.7  14.2       251.0      179.0  \n5328   30.5  11.2       251.0      167.0  \n\n[5 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>HR</th>\n      <th>O2Sat</th>\n      <th>Temp</th>\n      <th>SBP</th>\n      <th>MAP</th>\n      <th>DBP</th>\n      <th>Resp</th>\n      <th>EtCO2</th>\n      <th>BaseExcess</th>\n      <th>HCO3</th>\n      <th>...</th>\n      <th>Phosphate</th>\n      <th>Potassium</th>\n      <th>Bilirubin_total</th>\n      <th>TroponinI</th>\n      <th>Hct</th>\n      <th>Hgb</th>\n      <th>PTT</th>\n      <th>WBC</th>\n      <th>Fibrinogen</th>\n      <th>Platelets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17166</th>\n      <td>90.0</td>\n      <td>100.0</td>\n      <td>37.3</td>\n      <td>108.5</td>\n      <td>79.5</td>\n      <td>67.5</td>\n      <td>10.0</td>\n      <td>33.0</td>\n      <td>0.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>3.4</td>\n      <td>3.7</td>\n      <td>0.8</td>\n      <td>0.14</td>\n      <td>33.7</td>\n      <td>11.6</td>\n      <td>34.2</td>\n      <td>24.0</td>\n      <td>251.0</td>\n      <td>157.0</td>\n    </tr>\n    <tr>\n      <th>8962</th>\n      <td>91.0</td>\n      <td>98.0</td>\n      <td>36.8</td>\n      <td>137.0</td>\n      <td>82.0</td>\n      <td>62.0</td>\n      <td>21.0</td>\n      <td>33.0</td>\n      <td>0.0</td>\n      <td>27.0</td>\n      <td>...</td>\n      <td>3.8</td>\n      <td>4.3</td>\n      <td>0.7</td>\n      <td>0.14</td>\n      <td>40.4</td>\n      <td>13.9</td>\n      <td>38.6</td>\n      <td>8.3</td>\n      <td>251.0</td>\n      <td>209.0</td>\n    </tr>\n    <tr>\n      <th>10453</th>\n      <td>82.0</td>\n      <td>98.0</td>\n      <td>36.8</td>\n      <td>120.0</td>\n      <td>80.0</td>\n      <td>62.0</td>\n      <td>18.0</td>\n      <td>33.0</td>\n      <td>0.0</td>\n      <td>25.0</td>\n      <td>...</td>\n      <td>4.2</td>\n      <td>5.1</td>\n      <td>0.5</td>\n      <td>0.14</td>\n      <td>31.4</td>\n      <td>9.6</td>\n      <td>30.7</td>\n      <td>7.2</td>\n      <td>251.0</td>\n      <td>585.0</td>\n    </tr>\n    <tr>\n      <th>19087</th>\n      <td>66.0</td>\n      <td>100.0</td>\n      <td>37.6</td>\n      <td>124.0</td>\n      <td>78.0</td>\n      <td>58.0</td>\n      <td>16.0</td>\n      <td>31.0</td>\n      <td>0.0</td>\n      <td>24.0</td>\n      <td>...</td>\n      <td>3.4</td>\n      <td>4.1</td>\n      <td>0.8</td>\n      <td>0.14</td>\n      <td>26.9</td>\n      <td>9.0</td>\n      <td>30.7</td>\n      <td>14.2</td>\n      <td>251.0</td>\n      <td>179.0</td>\n    </tr>\n    <tr>\n      <th>5328</th>\n      <td>103.0</td>\n      <td>100.0</td>\n      <td>36.8</td>\n      <td>108.0</td>\n      <td>82.0</td>\n      <td>68.0</td>\n      <td>18.0</td>\n      <td>33.0</td>\n      <td>0.0</td>\n      <td>25.0</td>\n      <td>...</td>\n      <td>2.4</td>\n      <td>3.5</td>\n      <td>0.8</td>\n      <td>0.14</td>\n      <td>23.7</td>\n      <td>8.4</td>\n      <td>30.5</td>\n      <td>11.2</td>\n      <td>251.0</td>\n      <td>167.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the trained imputer to transform the training set replacing the\n",
    "# missing values with learn medians\n",
    "N = num_imputer.transform(data_num)\n",
    "# result above is plain NumPy array with transformed features\n",
    "# put back to a pandas DataFrame\n",
    "num_tr = pd.DataFrame(N, columns=data_num.columns, index=data_num.index)\n",
    "num_tr.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Scaling\n",
    "1. ML algorithms don't work well when numeric attributes have very different scales\n",
    "    (e.g. HR max 184,  pH max 7.67)\n",
    "1. Scaling target values is not necessary\n",
    "1. Apply\n",
    "    1. normalization (MinMaxScaler) bounds the values to a specific range (e.g. 0-1)\n",
    "    1. standardization (StandardScaler) less affected by outliers does not bound to range\n",
    "\n",
    "### Transformation Pipeline\n",
    "\n",
    "1. Common to apply many transformation steps in a specific order"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "                        ('imputer', SimpleImputer(strategy='median')),\n",
    "                        ('std_scaler', StandardScaler()),\n",
    "                        ])\n",
    "\n",
    "num_prepared = num_pipeline.fit_transform(data_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Data Pipeline\n",
    "\n",
    "Single transformer to handle numeric and categorical columns using ColumnTransformer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# get a list of numeric column names\n",
    "num_attribs = list(data_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# construct the transformer\n",
    "full_pipeline = ColumnTransformer([\n",
    "    # transform number columns with num_pipeline defined earlier\n",
    "    (\"num\", num_pipeline, num_attribs)\n",
    "])\n",
    "\n",
    "# only run the pipeline on the training as the test data will be applied during the evaluation stage with the final model\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Selection\n",
    "\n",
    "![image](images/scikitlearn-choose-right-estimator.png)\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "## Classifier Comparison\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html?highlight=svm%20svc\n",
    "\n",
    "1. [Linear Support Vector Machine \"SVM\" Support Vector Classifier \"SVC\"](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "1. [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "1. [K-Neighbors Classifier](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification) implements learning based on the  nearest neighbors of each query point, where  is an integer value specified by the user.\n",
    "1. [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "1. [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) a linear model for classification\n",
    "1. [Stochastic Gradient Descent \"SGD\" Classifier](https://scikit-learn.org/stable/modules/sgd.html#classification)\n",
    "1. [Neural Network Multi-Layer Perceptron Classifier](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron) a supervised learning algorithm that learns a function  by training on a dataset\n",
    "1. [XGBoost Classifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=xgbclassifier#xgboost.XGBClassifier)\n",
    "\n",
    "## Scoring\n",
    "https://arifromadhan19.medium.com/part-1-regression-and-classification-model-evaluation-bc7f6ab3b4dd\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html#\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "Scoring metrics for Classification models\n",
    "1. Classification\n",
    "1. Accuracy\n",
    "1. Precision\n",
    "1. Recall\n",
    "1. Specificity\n",
    "1. F1 Score\n",
    "\n",
    "Best practice to save every model you experiment with so you can come back easily to any model.\n",
    "Save both the hyperparameters and trained parameter, as well as the cross-validation scores and predictions.\n",
    "This will allow you to easily compare scores across model types. Use Pickle or joblib libraries."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear SVM SVC\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   46.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([11.9992919 , 12.64642811, 13.73776317]),\n 'score_time': array([10.42156386, 10.00926685,  9.93839383]),\n 'test_score': array([0.10377358, 0.10613208, 0.11254396]),\n 'train_score': array([0.20697413, 0.25655022, 0.2140056 ])}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the SVM\n",
    "lin_svm = svm.SVC()\n",
    "# fit the model to the data\n",
    "lin_svm.fit(X_train_prepared, y_train)\n",
    "# configure the cross validation\n",
    "cv_lin_svm = cross_validate(lin_svm, # estimator to fit\n",
    "                            X_train_prepared, # data to fit\n",
    "                            y_train, # target variable isSepsis\n",
    "                            n_jobs=-1, # use all the processors in parallel\n",
    "                            verbose=1, # verbosity level\n",
    "                            cv=3, # splitting strategy to compute the score N consecutive times with different splits\n",
    "                            scoring=\"f1\", # for binary targets\n",
    "                            return_train_score=True)\n",
    "\n",
    "# display the scoring\n",
    "cv_lin_svm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/lin_svm.pkl']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(lin_svm, \"models/experiment/lin_svm.pkl\")\n",
    "# reference to load the model\n",
    "#lin_svm_loaded = joblib.load(\"model/experiment/lin_svm.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.04836202, 0.03323007, 0.03448606]),\n 'score_time': array([0.00887394, 0.00911283, 0.00915408]),\n 'test_score': array([0.28099174, 0.23349181, 0.27734171]),\n 'train_score': array([0.27586207, 0.27238606, 0.26259542])}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_prepared, y_train)\n",
    "cv_n_bayes = cross_validate(gnb, # estimator to fit\n",
    "                            X_train_prepared, # data to fit\n",
    "                            y_train, # target variable isSepsis\n",
    "                            n_jobs=-1, # use all the processors in parallel\n",
    "                            verbose=1, # verbosity level\n",
    "                            cv=3, # splitting strategy to compute the score N consecutive times with different splits\n",
    "                            scoring=\"f1\", # for binary targets\n",
    "                            return_train_score=True)\n",
    "\n",
    "# display the scoring\n",
    "cv_n_bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/gnb.pkl']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(gnb, \"models/experiment/gnb.pkl\")\n",
    "# reference to load the model\n",
    "#gnb_loaded = joblib.load(\"model/experiment/gnb.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K Nearest Neighbor Classification\n",
    "https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   22.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.01612735, 0.01470995, 0.01478386]),\n 'score_time': array([8.79333568, 9.93717027, 9.89686728]),\n 'test_score': array([0.35692771, 0.40208488, 0.38897638]),\n 'train_score': array([0.52214452, 0.51745068, 0.50679084])}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_prepared, y_train)\n",
    "cv_knn = cross_validate(knn,\n",
    "                        X_train_prepared,\n",
    "                        y_train,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1,\n",
    "                        cv=3,\n",
    "                        scoring=\"f1\",\n",
    "                        return_train_score=True)\n",
    "cv_knn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/knn.pkl']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(knn, \"models/experiment/knn.pkl\")\n",
    "# reference to load the model\n",
    "#knn_loaded = joblib.load(\"model/experiment/knn.pkl\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'fit_time': array([3.65996075, 3.70988512, 3.73199224]),\n 'score_time': array([0.13547111, 0.13466406, 0.13094187]),\n 'test_score': array([0.56818182, 0.54530478, 0.5728    ]),\n 'train_score': array([0.99905571, 0.99937028, 0.99937028])}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "cv_rf = cross_validate(rf,\n",
    "                       X_train_prepared,\n",
    "                       y_train,\n",
    "                       n_jobs=-1,\n",
    "                       cv=3,\n",
    "                       scoring=\"f1\",\n",
    "                       return_train_score=True)\n",
    "cv_rf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/rf.pkl']"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(rf, \"models/experiment/rf.pkl\")\n",
    "# reference to load the model\n",
    "#rf_loaded = joblib.load(\"model/experiment/rf.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "- [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.11095405, 0.14168811, 0.11711001, 0.12011695, 0.15819907]),\n 'score_time': array([0.00465989, 0.00610089, 0.004462  , 0.00369787, 0.0052979 ]),\n 'test_score': array([0.02424242, 0.02414487, 0.03225806, 0.04048583, 0.02469136]),\n 'train_score': array([0.03245436, 0.0364557 , 0.03444782, 0.03144016, 0.03134479])}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a variable to Logistic regression with verbosity\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_prepared, y_train)\n",
    "cv_log_reg = cross_validate(log_reg,\n",
    "                            X_train_prepared, # attributes\n",
    "                            y_train, # labels isSepsis\n",
    "                            n_jobs=-1, # use all the processors in parallel\n",
    "                            verbose=1, # verbosity level\n",
    "                            cv=5, # splitting strategy to compute the score N consecutive times with different splits\n",
    "                            scoring=\"f1\", # for binary targets\n",
    "                            return_train_score=True) # computationally expensive, whether to include training scores on parameters impact\n",
    "cv_log_reg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/log_reg.pkl']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(log_reg, \"models/experiment/log_reg.pkl\")\n",
    "# reference to load the model\n",
    "#log_reg_loaded = joblib.load(\"model/experiment/log_reg.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SGDClassifier\n",
    "\n",
    "advantages\n",
    "- efficient\n",
    "\n",
    "disadvantages\n",
    "- sensitive to feature scaling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([0.63595223, 0.73067617, 0.70459986, 0.58786821, 0.32503796]),\n 'score_time': array([0.00507975, 0.00321102, 0.00374603, 0.00774407, 0.0035522 ]),\n 'test_score': array([0.00819672, 0.02811245, 0.05882353, 0.01629328, 0.02061856]),\n 'train_score': array([0.01427843, 0.03923541, 0.05135802, 0.02135231, 0.02755102])}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(loss=\"log\", # logistic regression\n",
    "                        penalty=\"elasticnet\",\n",
    "                        shuffle=True,\n",
    "                        learning_rate='optimal')\n",
    "sgd_clf.fit(X_train_prepared, y_train)\n",
    "cv_sgd_clf = cross_validate(sgd_clf,\n",
    "                            X_train_prepared, # attributes\n",
    "                            y_train, # labels isSepsis\n",
    "                            n_jobs=-1, # use all the processors in parallel\n",
    "                            verbose=1, # verbosity level\n",
    "                            cv=5, # splitting strategy to compute the score N consecutive times with different splits\n",
    "                            scoring=\"f1\", # for binary targets\n",
    "                            return_train_score=True) # computationally expensive, whether to include training scores on parameters impact\n",
    "\n",
    "cv_sgd_clf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/sgd_clf.pkl']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# serialize the model\n",
    "joblib.dump(sgd_clf, \"models/experiment/sgd_clf.pkl\")\n",
    "# reference load the model\n",
    "#sgd_clf_loaded = joblib.load(\"model/experiment/sgd_clf.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 10.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([179.28927207, 224.22065091, 208.5081861 ]),\n 'score_time': array([0.02683997, 0.02259612, 0.025383  ]),\n 'test_score': array([0.39190898, 0.40836408, 0.38754765]),\n 'train_score': array([0.96990814, 0.97395172, 0.97684745])}"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = MLPClassifier(solver='sgd', # For small datasets, ‘lbfgs’ can converge faster and perform better.\n",
    "                   activation='relu',\n",
    "                   max_iter=5000, # The solver iterates until convergence\n",
    "                   hidden_layer_sizes=(50,50,50,50), # The ith element represents the number of neurons in the ith hidden layer\n",
    "                   verbose=0,\n",
    "                   learning_rate=\"adaptive\") #  keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing\n",
    "cv_nn = cross_validate(nn,\n",
    "                       X_train_prepared,\n",
    "                       y_train,\n",
    "                       cv=3,\n",
    "                       scoring=\"f1\",\n",
    "                       return_train_score=True,\n",
    "                       verbose=1)\n",
    "cv_nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/nn.pkl']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(nn, \"models/experiment/nn.pkl\")\n",
    "# reference to load the model\n",
    "#nn_loaded = joblib.load(\"model/experiment/nn.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost Classifier\n",
    "\n",
    "Learning task parameter = https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "{'fit_time': array([2.43767905, 1.3382349 , 1.28723097]),\n 'score_time': array([0.01427674, 0.01267815, 0.01388526]),\n 'test_score': array([0.54941634, 0.55221519, 0.58578053]),\n 'train_score': array([0.88031652, 0.88624339, 0.86704432])}"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost = XGBClassifier(use_label_encoder=False, # removes user warning error\n",
    "                        booster=\"gbtree\",\n",
    "                        eval_metric='mlogloss')\n",
    "cv_xgboost = cross_validate(xgboost,\n",
    "                            X_train_prepared,\n",
    "                            y_train,\n",
    "                            cv=3,\n",
    "                            scoring=\"f1\",\n",
    "                            return_train_score=True,\n",
    "                            verbose=0)\n",
    "cv_xgboost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/xgb.pkl']"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(xgboost, \"models/experiment/xgb.pkl\")\n",
    "# reference to load the model\n",
    "#xgboost_loaded = joblib.load(\"models/experiment/xgboost.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tune the model\n",
    "Instead of messing with hyperparameters manually, GridSearchCV can be instructed to search hyperparameters\n",
    "and uses cross validation to evaluate the possible combinations.\n",
    "\n",
    "During this sampling cycle, you may go back to your pipeline and:\n",
    "1. drop uninformative features\n",
    "1. add extra features\n",
    "1. clean up outliers\n",
    "\n",
    "### Grid Search on Random Forest Classifier\n",
    "\n",
    "[sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest%20classifier#sklearn.ensemble.RandomForestClassifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n             param_grid=[{'max_features': [10, 15, 20, 30],\n                          'n_estimators': [3, 10, 30, 100]},\n                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n                          'n_estimators': [3, 10]}],\n             return_train_score=True, scoring='f1')"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_param_grid = [\n",
    "    {'n_estimators': [5, 10, 15, 20], # The number of trees in the forest.\n",
    "     'max_features': [10, 15, 20, 30]}, # The number of features to consider when looking for the best split\n",
    "    {'bootstrap': [False], # If False, the whole dataset is used to build each tree\n",
    "     'n_estimators': [3,10],\n",
    "     'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# the grid search will explore 4 x 4 combinations of Random Forest Classifier combinations of\n",
    "# n_estimators and max_features hyperparameters values\n",
    "# Then will try 2 x 3 combinations of hyperparameter values in the second dictionary\n",
    "# with the bootstrap set to false\n",
    "# overall, gridsearch will explore 16 + 6 = 22 combinations of RFC hyperparameters and train each modele 3 times\n",
    "rfc_grid_search = GridSearchCV(rfc,\n",
    "                           rfc_param_grid,\n",
    "                           cv=3,\n",
    "                           scoring='f1',\n",
    "                           return_train_score=True)\n",
    "\n",
    "rfc_grid_search.fit(X_train_prepared, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5524979689752308"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best score\n",
    "rfc_grid_search.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "{'max_features': 15, 'n_estimators': 100}"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best combination of parameters\n",
    "rfc_grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(max_features=15)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best estimator directly\n",
    "rfc_grid_search.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.10080489989104702, 'Temp'),\n (0.07748042225765787, 'WBC'),\n (0.07641134866426043, 'BUN'),\n (0.061636673640003296, 'HR'),\n (0.057731118320587686, 'Hgb'),\n (0.05487352507261766, 'Resp'),\n (0.0511490511755732, 'Hct'),\n (0.05073491288456084, 'SBP'),\n (0.04809497355457121, 'Creatinine'),\n (0.047273093695356164, 'DBP'),\n (0.047027014105542865, 'MAP'),\n (0.032694465524768526, 'Platelets'),\n (0.031886427038485037, 'O2Sat'),\n (0.031523365985405505, 'Glucose'),\n (0.028310601425176867, 'Calcium'),\n (0.021407485312965217, 'Potassium'),\n (0.01849600464681473, 'Chloride'),\n (0.01768530092800006, 'PaCO2'),\n (0.017316501799187912, 'Phosphate'),\n (0.016840235648914022, 'pH'),\n (0.01572838364471813, 'HCO3'),\n (0.015127202730327798, 'Magnesium'),\n (0.013709013529501124, 'PTT'),\n (0.013706304936230885, 'BaseExcess'),\n (0.01245806056581834, 'Lactate'),\n (0.011807303338982859, 'Bilirubin_total'),\n (0.00878999623077239, 'AST'),\n (0.008657667790379896, 'Alkalinephos'),\n (0.005467750135777415, 'Gender'),\n (0.0051708955259951814, 'Gender')]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the importance scores next to the corresponding attributes\n",
    "# from this you can drop less useful features\n",
    "feature_importances = rfc_grid_search.best_estimator_.feature_importances_\n",
    "sorted(zip(feature_importances, num_attribs), reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid Search on XGBoost\n",
    "[xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=xgbclassifier#xgboost.XGBClassifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=3,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None,\n                                     eval_metric='mlogloss', gamma=None,\n                                     gpu_id=None, importance_type='gain',\n                                     interaction_constraints=None,\n                                     learning_rate=None, max_delta_step=None,\n                                     max_depth=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,...\n                                     num_parallel_tree=None, random_state=None,\n                                     reg_alpha=None, reg_lambda=None,\n                                     scale_pos_weight=None, subsample=None,\n                                     tree_method=None, use_label_encoder=False,\n                                     validate_parameters=None, verbosity=None),\n             param_grid={'alpha': [0, 0.1], 'max_delta_step': [0.1],\n                         'n_estimators': [150, 200], 'reg_lambda': [1, 1.1],\n                         'subsample': [None, 0.5, 1]},\n             return_train_score=True, scoring='f1')"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_param_grid = {\"n_estimators\": [150, 200],\n",
    "                  \"max_delta_step\": [0.1],\n",
    "                  \"subsample\": [None, 0.5, 1],\n",
    "                  \"reg_lambda\": [1, 1.1],\n",
    "                  \"alpha\": [0, 0.1]}\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "xgb_grid_search = GridSearchCV(xgb,\n",
    "                           xgb_param_grid,\n",
    "                           cv=3,\n",
    "                           scoring='f1',\n",
    "                           return_train_score=True)\n",
    "\n",
    "xgb_grid_search.fit(X_train_prepared, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5563996653758968"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best score\n",
    "xgb_grid_search.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "{'alpha': 0.1,\n 'max_delta_step': 0.1,\n 'n_estimators': 200,\n 'reg_lambda': 1,\n 'subsample': None}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best combination of parameters\n",
    "xgb_grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "XGBClassifier(alpha=0.1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, eval_metric='mlogloss',\n              gamma=0, gpu_id=-1, importance_type='gain',\n              interaction_constraints='', learning_rate=0.300000012,\n              max_delta_step=0.1, max_depth=6, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=200, n_jobs=12,\n              num_parallel_tree=1, random_state=0, reg_alpha=0.100000001,\n              reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', use_label_encoder=False,\n              validate_parameters=1, verbosity=None)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best estimator directly\n",
    "xgb_grid_search.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.17511103, 'BUN'),\n (0.16019359, 'Hgb'),\n (0.1014349, 'WBC'),\n (0.06369768, 'Hct'),\n (0.05132545, 'Platelets'),\n (0.035577442, 'Creatinine'),\n (0.029773762, 'Resp'),\n (0.026058856, 'Temp'),\n (0.024878712, 'Gender'),\n (0.022663962, 'DBP'),\n (0.02184942, 'HCO3'),\n (0.02081274, 'HR'),\n (0.019774554, 'SBP'),\n (0.018967059, 'BaseExcess'),\n (0.018740606, 'MAP'),\n (0.018679056, 'Potassium'),\n (0.017870182, 'pH'),\n (0.017260818, 'PaCO2'),\n (0.01718195, 'Magnesium'),\n (0.015856504, 'Calcium'),\n (0.015337686, 'Chloride'),\n (0.014651926, 'PTT'),\n (0.014223329, 'O2Sat'),\n (0.014109537, 'Alkalinephos'),\n (0.013986062, 'Glucose'),\n (0.013553107, 'Bilirubin_total'),\n (0.012698767, 'Phosphate'),\n (0.012648032, 'Lactate'),\n (0.01108327, 'AST'),\n (0.0, 'Gender')]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = xgb_grid_search.best_estimator_.feature_importances_\n",
    "sorted(zip(feature_importances, num_attribs), reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate RFC model on the Test set\n",
    "\n",
    "After tweaking the model, you can evaluate the final model on the test set.\n",
    "1. get the predictors and labels from your test set\n",
    "1. run the transformation pipeline with transform() < don't fit the test set\n",
    "1. run the prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# transform, DON'T fit the final data\n",
    "X_test_ready = full_pipeline.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "final_rfc = rfc_grid_search.best_estimator_\n",
    "\n",
    "rfc_predictions = final_rfc.predict(X_test_ready)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9476728174056733"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the percent of the predictions that were correct\n",
    "accuracy_score(y_test, rfc_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42641509433962266\n",
      "0.5432692307692307\n"
     ]
    }
   ],
   "source": [
    "# we can successfully identify 6 out of 10 patients that will develop sepsis in the next 6 days\n",
    "print(recall_score(y_test, rfc_predictions))\n",
    "print(f1_score(y_test, rfc_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/final_rfc.pkl']"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(final_rfc, \"models/final_rfc.pkl\")\n",
    "# reference to load the model\n",
    "#final_rfc_loaded = joblib.load(\"models/final_rfc.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate XGB model on the Test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "final_xgb = xgb_grid_search.best_estimator_\n",
    "\n",
    "xgb_predictions = final_xgb.predict(X_test_ready)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9493252547507574"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the accuracy\n",
    "accuracy_score(y_test, xgb_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4528301886792453\n",
      "0.5660377358490566\n"
     ]
    }
   ],
   "source": [
    "# we can successfully identify 6 out of 10 patients that will develop sepsis in the next 6 days\n",
    "print(recall_score(y_test, xgb_predictions))\n",
    "print(f1_score(y_test, xgb_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "['models/final_xgb.pkl']"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(final_xgb, \"models/final_xgb.pkl\")\n",
    "# reference to load the model\n",
    "#final_xgb_loaded = joblib.load(\"models/final_xgb.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Launch, Monitor, and Maintain\n",
    "\n",
    "1. Use joblib to save the train model inclulding full pre-processing and prediction pipeline\n",
    "1. Load the trained model to production\n",
    "1. Call the predict() method to make predictions\n",
    "\n",
    "## Serving\n",
    "1. Load the model in a web app that will call the predict() method\n",
    "1. Wrap the model in a dedicated web service that a web app queries with REST API\n",
    "    1. makes it easy to upgrade without interrupting the primary web app\n",
    "    1. makes it easy to scale web services and load balance the requests from the web app across the web services\n",
    "    1. enables the web app to use any language, not just Python\n",
    "\n",
    "## Monitor\n",
    "1. Write monitor code the check live performance at regular intervals and trigger alerts when it drops\n",
    "    1. Could be steep drop if an infrastructure components stops\n",
    "    1. or, a gentle decay as the world changes resulting in model rot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}